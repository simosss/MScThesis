{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simos\\Anaconda3\\envs\\myclone\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 7050). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Finished with creation of dictionaries\n",
      "Finished creating features\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_pos_edge_index = data.edge_index\n",
    "data.val_pos_edge_index = data.edge_index\n",
    "data.test_pos_edge_index = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 43953], edge_type=[43953], test_pos_edge_index=[2, 43953], train_pos_edge_index=[2, 43953], val_pos_edge_index=[2, 43953], x=[7736, 10184])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1.,  ..., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[:,380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sampling(edge_index):\n",
    "    struc_neg_sampl = pyg_utils.structured_negative_sampling(edge_index)\n",
    "    i,j,k = struc_neg_sampl\n",
    "    i = i.tolist()\n",
    "    k = k.tolist()\n",
    "    neg_edge_index = [i,k]\n",
    "    neg_edge_index = torch.tensor(neg_edge_index)\n",
    "    return neg_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 32)\n",
    "        self.R = torch.empty(32, 32)\n",
    "        self.D = torch.empty(32, 32)\n",
    "        nn.init.xavier_uniform_(self.R, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.D, gain=nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def encode(self):\n",
    "        x = self.conv1(data.x, data.train_pos_edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, data.train_pos_edge_index)\n",
    "        return x \n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "        return logits\n",
    "    \n",
    "    def decode_decagon(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
    "        logits = z[pos_edge_index[0]]@self.D@self.R@self.D@z[pos_edge_index[1]].t()\n",
    "        return logits\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    pos_edge_index = data.train_pos_edge_index\n",
    "    neg_edge_index  = neg_sampling(data.train_pos_edge_index)\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode()\n",
    "    link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
    "    link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "\n",
    "    pos_edge_index = data.val_pos_edge_index\n",
    "    neg_edge_index = neg_sampling(data.val_pos_edge_index)\n",
    "\n",
    "    z = model.encode()\n",
    "    link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
    "    link_probs = link_logits.sigmoid()\n",
    "    link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "    perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))\n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 4.8870, Val: 0.4592\n",
      "Epoch: 002, Loss: 124.0708, Val: 0.1806\n",
      "Epoch: 003, Loss: 15.2232, Val: 0.1998\n",
      "Epoch: 004, Loss: 7.3334, Val: 0.1965\n",
      "Epoch: 005, Loss: 2.3630, Val: 0.8494\n",
      "Epoch: 006, Loss: 0.7082, Val: 0.8357\n",
      "Epoch: 007, Loss: 0.6844, Val: 0.8357\n",
      "Epoch: 008, Loss: 0.6493, Val: 0.8372\n",
      "Epoch: 009, Loss: 0.5664, Val: 0.8376\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 10):\n",
    "   \n",
    "    train_loss = train()\n",
    "    val_perf = test()\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}'\n",
    "    print(log.format(epoch, train_loss, val_perf[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.4921, Val: 0.8384\n",
      "Epoch: 011, Loss: 0.4854, Val: 0.8377\n",
      "Epoch: 012, Loss: 0.5158, Val: 0.8387\n",
      "Epoch: 013, Loss: 0.5323, Val: 0.8379\n",
      "Epoch: 014, Loss: 0.5464, Val: 0.8381\n",
      "Epoch: 015, Loss: 0.5522, Val: 0.8382\n",
      "Epoch: 016, Loss: 0.5374, Val: 0.8382\n",
      "Epoch: 017, Loss: 0.5207, Val: 0.8384\n",
      "Epoch: 018, Loss: 0.5022, Val: 0.8386\n",
      "Epoch: 019, Loss: 0.4871, Val: 0.8380\n",
      "Epoch: 020, Loss: 0.4825, Val: 0.8387\n",
      "Epoch: 021, Loss: 0.4922, Val: 0.9379\n",
      "Epoch: 022, Loss: 0.5074, Val: 0.9513\n",
      "Epoch: 023, Loss: 0.5115, Val: 0.9564\n",
      "Epoch: 024, Loss: 0.5022, Val: 0.9605\n",
      "Epoch: 025, Loss: 0.4898, Val: 0.9605\n",
      "Epoch: 026, Loss: 0.4801, Val: 0.9628\n",
      "Epoch: 027, Loss: 0.4807, Val: 0.9635\n",
      "Epoch: 028, Loss: 0.4869, Val: 0.9633\n",
      "Epoch: 029, Loss: 0.4869, Val: 0.9640\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10, 30):\n",
    "   \n",
    "    train_loss = train()\n",
    "    val_perf = test()\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}'\n",
    "    print(log.format(epoch, train_loss, val_perf[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0137,  0.0320,  0.0415,  0.0157,  0.0193,  0.0637,  0.0996,  0.0450,\n",
      "         0.0453,  0.0759,  0.0991,  0.0159,  0.0595,  0.0236,  0.0296,  0.1058,\n",
      "         0.0614,  0.1262,  0.0426,  0.0398,  0.0866,  0.0457,  0.0494,  0.0525,\n",
      "         0.0255,  0.0566,  0.0291,  0.0120,  0.0369,  0.0225,  0.0867,  0.0799,\n",
      "         0.1061,  0.0739,  0.0440,  0.0278,  0.0954,  0.0448,  0.1055,  0.0296,\n",
      "         0.0428,  0.0197,  0.0227,  0.0250,  0.1018,  0.0745,  0.0570,  0.0285,\n",
      "         0.0484,  0.0535,  0.0328,  0.0882,  0.0413,  0.0473,  0.0531,  0.0680,\n",
      "         0.0536,  0.0436,  0.0332,  0.1228,  0.0303,  0.0489,  0.0394,  0.0268,\n",
      "         0.0229,  0.0283,  0.1316,  0.1226,  0.0391,  0.0971,  0.0507,  0.1098,\n",
      "         0.0452,  0.0351,  0.1518,  0.0313,  0.0354,  0.0686,  0.0687,  0.0638,\n",
      "         0.0541,  0.0953,  0.0640,  0.0946,  0.1093,  0.0337,  0.0935,  0.0480,\n",
      "         0.0124,  0.0419,  0.0321,  0.0269,  0.0439,  0.1927,  0.0441,  0.0181,\n",
      "         0.0299,  0.0448,  0.1218,  0.0688,  0.0262,  0.0487,  0.1634,  0.1370,\n",
      "         0.0151,  0.0163,  0.0677,  0.0804,  0.1102,  0.1061,  0.1126,  0.0784,\n",
      "         0.0204,  0.0828,  0.0454,  0.0364,  0.0381,  0.0150,  0.0485,  0.0544,\n",
      "         0.0374,  0.1065,  0.0999,  0.0243,  0.0432,  0.0458,  0.0308,  0.0623,\n",
      "         0.0330,  0.0182,  0.1172,  0.0286,  0.1399,  0.0758,  0.0135,  0.0170,\n",
      "         0.0252,  0.0481,  0.0282,  0.0123,  0.0837,  0.0502,  0.0332,  0.0364,\n",
      "         0.0147,  0.0571,  0.0769,  0.0271,  0.0433,  0.0911,  0.0801,  0.1043,\n",
      "         0.0359,  0.0483,  0.1166,  0.0786,  0.0461,  0.1536,  0.0598,  0.0345,\n",
      "         0.0192,  0.0380,  0.1090,  0.0359,  0.0373,  0.1120,  0.1398,  0.2457,\n",
      "         0.0285,  0.0142,  0.0257,  0.1212,  0.0616,  0.1962,  0.0711,  0.0516,\n",
      "         0.0107,  0.0916,  0.0680,  0.0588,  0.1469,  0.0897,  0.1435,  0.0995,\n",
      "         0.0731,  0.0221,  0.0103,  0.0354,  0.0629,  0.0698,  0.0718,  0.1195,\n",
      "         0.0736,  0.1140,  0.0545,  0.0288,  0.1400,  0.0144,  0.1191,  0.1156,\n",
      "         0.0209,  0.0656,  0.1306,  0.0284,  0.0684,  0.0604,  0.0690,  0.0487,\n",
      "         0.0393,  0.0949,  0.1105,  0.1086,  0.0629,  0.0941,  0.0708,  0.1285,\n",
      "         0.0156,  0.0986,  0.0923,  0.1286,  0.0940,  0.0511,  0.0129,  0.1031,\n",
      "         0.0964,  0.0599,  0.1743,  0.0902,  0.1406,  0.0753,  0.0551,  0.0555,\n",
      "         0.1705,  0.0252,  0.0440,  0.0510,  0.1130,  0.0676,  0.1723,  0.1071,\n",
      "         0.0572,  0.0720,  0.1400,  0.0104,  0.1086,  0.0490,  0.1940,  0.1481,\n",
      "         0.0964,  0.1296,  0.1215,  0.1386,  0.0369,  0.2467,  0.0763,  0.1174,\n",
      "         0.0753,  0.0337,  0.1438,  0.0918,  0.1224,  0.0132,  0.0674,  0.0730,\n",
      "         0.1769,  0.1415,  0.0355,  0.1444,  0.0953,  0.1753,  0.0749,  0.0892,\n",
      "         0.0411,  0.1160,  0.0239,  0.0769,  0.0341,  0.0646,  0.0320,  0.0537,\n",
      "         0.0843,  0.0113,  0.0324,  0.0303,  0.0202,  0.0154,  0.1786,  0.1608,\n",
      "         0.0334,  0.1384,  0.0802,  0.1505,  0.1147,  0.0791,  0.0775,  0.1049,\n",
      "         0.0360,  0.0367,  0.0854,  0.0755,  0.1486,  0.2038,  0.1565,  0.1492,\n",
      "         0.0524,  0.1036,  0.0342,  0.0468,  0.0381,  0.0735,  0.1344,  0.0764,\n",
      "         0.0549,  0.0397,  0.0503,  0.1081,  0.0444,  0.0563,  0.0282,  0.0876,\n",
      "         0.0280,  0.0297,  0.0296,  0.0391,  0.0918,  0.1116,  0.0645,  0.1078,\n",
      "         0.1783,  0.0982,  0.0475,  0.0204,  0.1377,  0.0484,  0.0422,  0.0320,\n",
      "         0.0735,  0.0564,  0.0364,  0.1242,  0.1563,  0.1442,  0.1611,  0.1131,\n",
      "         0.1048,  0.0222,  0.0467,  0.1970,  0.1337,  0.0556,  0.1931,  0.0733,\n",
      "         0.0223,  0.1008,  0.0409,  0.0354,  0.1274,  0.0185,  0.0378,  0.1003,\n",
      "         0.0128,  0.0233,  0.1078,  0.1736,  0.2198,  0.1244,  0.0806,  0.1662,\n",
      "         0.0677,  0.0376,  0.0315,  0.0661,  0.0760,  0.1408,  0.0888,  0.1435,\n",
      "         0.0967,  0.0669,  0.0652,  0.0271,  0.1004,  0.0656,  0.0914,  0.0120,\n",
      "         0.0618,  0.0294,  0.0808,  0.0469,  0.0455,  0.1093,  0.0950,  0.1191,\n",
      "         0.0460,  0.0523,  0.0223,  0.0154,  0.0240,  0.0847,  0.0276,  0.0760,\n",
      "         0.0840,  0.0417,  0.1118,  0.0311,  0.1365,  0.0613,  0.1307,  0.0618,\n",
      "         0.1158,  0.0857,  0.0272,  0.1081,  0.1546,  0.0447,  0.0973,  0.1049,\n",
      "         0.1697,  0.0508,  0.1162,  0.1284,  0.1400,  0.0782,  0.1131,  0.0989,\n",
      "         0.0625,  0.0493,  0.1795,  0.1627,  0.1126,  0.0300,  0.0680,  0.1705,\n",
      "         0.0592,  0.0818,  0.0297,  0.0286,  0.0897,  0.1335,  0.1678,  0.0214,\n",
      "         0.0714,  0.0416,  0.0291,  0.0383,  0.0609,  0.1288,  0.2335,  0.1270,\n",
      "         0.2063,  0.1326,  0.1198,  0.1011,  0.1671,  0.0718,  0.0567,  0.0852,\n",
      "         0.0443,  0.0278,  0.0372,  0.0332,  0.0435,  0.1069,  0.1351,  0.1103,\n",
      "         0.0754,  0.0192,  0.0329,  0.1269,  0.0956,  0.1040,  0.0603,  0.0757,\n",
      "         0.1880,  0.0796,  0.1982,  0.0136,  0.0332,  0.0696,  0.1180,  0.0290,\n",
      "         0.1471,  0.0236,  0.0356,  0.0642,  0.0401,  0.1153,  0.1602,  0.0065,\n",
      "         0.0370,  0.1325,  0.0169,  0.0856,  0.0303,  0.2024,  0.0189,  0.0218,\n",
      "         0.1887,  0.1841,  0.1282,  0.0800,  0.0614,  0.1169,  0.1689,  0.1516,\n",
      "         0.2203,  0.0278,  0.0180,  0.1030,  0.1557,  0.1609,  0.0519,  0.0603,\n",
      "         0.0298,  0.0545,  0.0181,  0.0586,  0.0265,  0.0456,  0.0441,  0.0505,\n",
      "         0.0531,  0.0281,  0.1582,  0.0410,  0.1570,  0.1388,  0.0840,  0.1833,\n",
      "         0.0511,  0.0477,  0.0244,  0.0277,  0.1378,  0.0970,  0.0532,  0.0585,\n",
      "         0.0951,  0.0312,  0.1152,  0.1156,  0.0031,  0.0940,  0.1486,  0.0833,\n",
      "         0.0429,  0.1750,  0.0184,  0.0669,  0.0300,  0.0352,  0.0789,  0.0270,\n",
      "         0.0494,  0.0475,  0.0609,  0.0468,  0.0919,  0.0455,  0.0221,  0.0564,\n",
      "         0.0219,  0.1616,  0.1064,  0.0903,  0.0253,  0.0319,  0.1798,  0.0981,\n",
      "         0.0226,  0.0855,  0.1664,  0.2240,  0.3530,  0.0259,  0.0118,  0.0593,\n",
      "         0.0359,  0.0179,  0.0735,  0.1608,  0.0060,  0.1803,  0.1131,  0.0822,\n",
      "         0.0737,  0.1515,  0.0395,  0.0280,  0.1239,  0.0187,  0.0405,  0.0984,\n",
      "         0.2436,  0.1645,  0.0222,  0.0142,  0.2790,  0.0948,  0.0571,  0.0539,\n",
      "         0.0749,  0.2778,  0.0772,  0.0092,  0.0090,  0.1489,  0.0696,  0.0831,\n",
      "         0.0060,  0.0536,  0.0472,  0.0798,  0.1043,  0.0762,  0.0137,  0.1185,\n",
      "         0.0479,  0.2396,  0.2228,  0.0208,  0.2821,  0.0667,  0.0319,  0.0104,\n",
      "         0.0597,  0.0133, -0.0131,  0.0131, -0.1118,  0.3364,  0.0485,  0.0214,\n",
      "         0.0060,  ...,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0096,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0077,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0067,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0077,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0069,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0167,  0.0131,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0163,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0107,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,  0.0060,\n",
      "         0.0060,  0.0060,  0.0060], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(edgeitems=633)\n",
    "z = model.encode()\n",
    "print(z[:,22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
